{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnStore Bulk Data Adapters - Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries and set necessary configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from matplotlib import pyplot as plt\n",
    "import mysql.connector as mariadb\n",
    "import sys, decimal, datetime\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "url = 'jdbc:mysql://columnstore_host_nm:3306'\n",
    "properties = {'user': 'jupiter_user', 'password': 'jupiter_pass', 'driver': 'org.mariadb.jdbc.Driver'}\n",
    "\n",
    "sc = SparkContext(\"local\", \"MariaDB Spark ColumnStore Demo\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the images for classification\n",
    "\n",
    "We apply a trained random forest classification model [\\[1\\]](./Model_Training.ipynb) to the MNIST database of handwritten digits to determine and digitalize the digit written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the handwritten numbers to predict\n",
    "test = sqlContext.read.format(\"libsvm\").option(\"numFeatures\", \"784\").load(\"./mnist.t\")\n",
    "\n",
    "# output statistics\n",
    "print(\"We have %d test images.\" % test.count())\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizes the features vector\n",
    "def visualizeFeatures(features):\n",
    "    image = np.array(features, dtype='float')\n",
    "    pixels = image.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "for row in test.head(3):\n",
    "    visualizeFeatures(row.features)\n",
    "    print(\"visualization of image with label %d\" % (row.label,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the handwritten numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model to predict the numbers\n",
    "model = RandomForestClassificationModel.load(\"mnist-model-random-forest\")\n",
    "\n",
    "# predict the handwritten numbers\n",
    "predictions = model.transform(test)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first three results\n",
    "for row in predictions.head(3):\n",
    "    visualizeFeatures(row.features)\n",
    "    print(\"prediction: %d\\tconfidence: %f\\tlabel: %d\" % (row.prediction, row.probability[int(row.prediction)], row.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure the dataframe for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return (row.label, row.prediction) + tuple(row.probability.toArray().tolist())\n",
    "\n",
    "output = predictions.rdd.map(extract).toDF([\"label\",\"prediction\",\"prob_0\",\"prob_1\",\"prob_2\",\"prob_3\",\"prob_4\",\"prob_5\",\"prob_6\",\"prob_7\",\"prob_8\",\"prob_9\"])\n",
    "print(\"Number of predictions: %d\" % output.count())\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest dataframe through JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "limit = 1000\n",
    "\n",
    "output.limit(limit).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"numPartitions\", 1) \\\n",
    "    .option(\"createTableOptions\", \"ENGINE=columnstore\") \\\n",
    "    .option(\"createTableColumnTypes\", \"label double, prediction double, prob_0 double, prob_1 double, prob_2 double, prob_3 double, prob_4 double, prob_5 double, prob_6 double, prob_7 double, prob_8 double, prob_9 double\") \\\n",
    "    .jdbc(url, \"test.jdbc\", properties=properties)\n",
    "\n",
    "print(\"%d rows ingested in %.3fs\" % (limit, time.time() - t,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest dataframe through Bulk Data Adapter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table function\n",
    "def createTable(name):\n",
    "    try:\n",
    "        conn = mariadb.connect(user='jupiter_user', password='jupiter_pass', host='columnstore_host_nm', database='test')\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS %s \\\n",
    "                       (label double, prediction double, prob_0 double, prob_1 double, prob_2 double, prob_3 double, prob_4 double, prob_5 double, prob_6 double, prob_7 double, prob_8 double, prob_9 double)\\\n",
    "                       engine=columnstore\" %(name,))\n",
    "\n",
    "    except mariadb.Error as err:\n",
    "        print(\"Error while creating table %s. %s\" %(name,err,))\n",
    "    \n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if conn: conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymcsapi            \n",
    "\n",
    "#create table\n",
    "createTable(\"bulk_api_1\")\n",
    "\n",
    "# initialize the driver\n",
    "driver = pymcsapi.ColumnStoreDriver()\n",
    "bulk = driver.createBulkInsert('test', 'bulk_api_1', 0, 0)\n",
    "\n",
    "# insert the dataframe row by row into ColumnStore\n",
    "for row in output.collect():\n",
    "    bulk.setColumn(0, row.label)\n",
    "    bulk.setColumn(1, row.prediction)\n",
    "    bulk.setColumn(2, row.prob_0)\n",
    "    bulk.setColumn(3, row.prob_1)\n",
    "    bulk.setColumn(4, row.prob_2)\n",
    "    bulk.setColumn(5, row.prob_3)\n",
    "    bulk.setColumn(6, row.prob_4)\n",
    "    bulk.setColumn(7, row.prob_5)\n",
    "    bulk.setColumn(8, row.prob_6)\n",
    "    bulk.setColumn(9, row.prob_7)\n",
    "    bulk.setColumn(10, row.prob_8)\n",
    "    bulk.setColumn(11, row.prob_9)\n",
    "    bulk.writeRow()\n",
    "    \n",
    "# commit the changes\n",
    "bulk.commit()\n",
    "\n",
    "# show a summary\n",
    "summary = bulk.getSummary()\n",
    "print(\"Execution time: %s\" % (summary.getExecutionTime(),))\n",
    "print(\"Rows inserted: %s\" % (summary.getRowsInsertedCount(),))\n",
    "print(\"Truncation count: %s\" % (summary.getTruncationCount(),))\n",
    "print(\"Saturated count: %s\" % (summary.getSaturatedCount(),))\n",
    "print(\"Invalid count: %s\" % (summary.getInvalidCount(),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest through ColumnStoreExporter / SparkConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import columnStoreExporter\n",
    "\n",
    "createTable(\"bulk_api_2\")\n",
    "columnStoreExporter.export(\"test\",\"bulk_api_2\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkConnector in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(database, table, df):\n",
    "    \n",
    "    global long\n",
    "    python2 = True\n",
    "\n",
    "    if sys.version_info[0] == 3:\n",
    "        long = int\n",
    "        python2 = False\n",
    "\n",
    "    rows = df.collect()\n",
    "    driver = pymcsapi.ColumnStoreDriver()\n",
    "    bulkInsert = driver.createBulkInsert(database, table, 0, 0)\n",
    "    \n",
    "    # get the column count of table\n",
    "    dbCatalog = driver.getSystemCatalog()\n",
    "    dbTable = dbCatalog.getTable(database, table)\n",
    "    dbTableColumnCount = dbTable.getColumnCount()\n",
    "    \n",
    "    # insert row by row into table\n",
    "    try:\n",
    "        for row in rows:\n",
    "            for columnId in range(0, len(row)):\n",
    "                if columnId < dbTableColumnCount:\n",
    "                    if isinstance(row[columnId], bool):\n",
    "                        if row[columnId]:\n",
    "                            bulkInsert.setColumn(columnId, 1)\n",
    "                        else:\n",
    "                            bulkInsert.setColumn(columnId, 0)\n",
    "                    \n",
    "                    elif isinstance(row[columnId], datetime.date):\n",
    "                        bulkInsert.setColumn(columnId, row[columnId].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                    \n",
    "                    elif isinstance(row[columnId], decimal.Decimal):\n",
    "                        dbColumn = dbTable.getColumn(columnId)\n",
    "                        #DATA_TYPE_DECIMAL, DATA_TYPE_UDECIMAL, DATA_TYPE_FLOAT, DATA_TYPE_UFLOAT, DATA_TYPE_DOUBLE, DATA_TYPE_UDOUBLE\n",
    "                        if dbColumn.getType() == 4 or dbColumn.getType() == 18 or dbColumn.getType() == 7 or dbColumn.getType() == 21 or dbColumn.getType() == 10 or dbColumn.getType() == 23:\n",
    "                            s = '{0:f}'.format(row[columnId])\n",
    "                            bulkInsert.setColumn(columnId, pymcsapi.ColumnStoreDecimal(s))\n",
    "                        #ANY OTHER DATA TYPE\n",
    "                        else:\n",
    "                            bulkInsert.setColumn(columnId, long(row[columnId]))\n",
    "    \n",
    "                    #handle python2 unicode strings\n",
    "                    elif python2 and isinstance(row[columnId], unicode):\n",
    "                        bulkInsert.setColumn(columnId, row[columnId].encode('utf-8'))\n",
    "\n",
    "                    #any other datatype is inserted without parsing\n",
    "                    else:\n",
    "                        bulkInsert.setColumn(columnId, row[columnId])\n",
    "            bulkInsert.writeRow()\n",
    "        bulkInsert.commit()\n",
    "    except Exception as e:\n",
    "        bulkInsert.rollback()\n",
    "        print(row[columnId], type(row[columnId]))\n",
    "        print(type(e))\n",
    "        print(e)\n",
    "       \n",
    "    #print a short summary of the insertion process\n",
    "    summary = bulkInsert.getSummary()\n",
    "    print(\"Execution time: %s\" % (summary.getExecutionTime(),))\n",
    "    print(\"Rows inserted: %s\" % (summary.getRowsInsertedCount(),))\n",
    "    print(\"Truncation count: %s\" %(summary.getTruncationCount(),))\n",
    "    print(\"Saturated count: %s\" %(summary.getSaturatedCount(),))\n",
    "    print(\"Invalid count: %s\" %(summary.getInvalidCount(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createTable(\"bulk_api_3\")\n",
    "export(\"test\",\"bulk_api_3\",output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
