{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDBC API Write Benchmark\n",
    "\n",
    "Import needed packets and set connections up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.mariadb.columnstore.api.connector.ColumnStoreExporter\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.{SQLContext,DataFrame}\n",
    "import org.apache.spark.sql.functions.{rand, randn, sha1, sha2, md5}\n",
    "import java.sql.{DriverManager,Connection,Date,Timestamp,PreparedStatement,ResultSet,SQLException}\n",
    "\n",
    "val url = \"jdbc:mysql://columnstore_host_nm:3306\"\n",
    "\n",
    "var connectionProperties = new Properties()\n",
    "connectionProperties.put(\"user\", \"jupiter_user\")\n",
    "connectionProperties.put(\"password\", \"jupiter_pass\")\n",
    "connectionProperties.put(\"driver\", \"org.mariadb.jdbc.Driver\")\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// SampleDataframe size parameter:\n",
    "val asciiRange = 128\n",
    "val randRange = 1000\n",
    "val hashRange = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var connection: Connection = null\n",
    "try {\n",
    "      connection = DriverManager.getConnection(url, connectionProperties)\n",
    "      val statement = connection.createStatement\n",
    "      statement.executeQuery(\"\"\"DROP DATABASE IF EXISTS benchmark\"\"\")\n",
    "      statement.executeQuery(\"\"\"CREATE DATABASE IF NOT EXISTS benchmark\"\"\")\n",
    "    } catch {\n",
    "      case e: Exception => e.printStackTrace()\n",
    "    } finally {\n",
    "      connection.close()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sample dataframes to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val asciiDF = sc.makeRDD(0 until asciiRange).map(i => (i.toChar.toString, i)).toDF(\"ascii_representation\", \"number\").cache()\n",
    "asciiDF.count()\n",
    "asciiDF.printSchema()\n",
    "val randDF = sqlContext.range(0, randRange).withColumn(\"uniform\", rand(seed=23)).withColumn(\"normal\", randn(seed=42)).cache()\n",
    "randDF.count()\n",
    "randDF.printSchema()\n",
    "val tmpDF = sc.makeRDD(0 until hashRange).map(i => (i, i.toString)).toDF(\"number\", \"string\")\n",
    "tmpDF.registerTempTable(\"tempDF\")\n",
    "val hashDF = sqlContext.sql(\"SELECT number, sha1(string) AS sha1, sha2(string,256) AS sha256, sha2(string,512) AS sha512, md5(string) AS md5 FROM tempDF\").cache()\n",
    "hashDF.count()\n",
    "hashDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createColumnStoreAPITable(name: String, schema: String) : Unit = {\n",
    "  var connection: Connection = null\n",
    "  try {\n",
    "      connection = DriverManager.getConnection(\"jdbc:mysql://columnstore_host_nm:3306/benchmark\", connectionProperties)\n",
    "      val statement = connection.createStatement\n",
    "      statement.executeQuery(\"CREATE TABLE IF NOT EXISTS \" + name + \" (\" + schema + \") engine=columnstore\")\n",
    "  } catch {\n",
    "      case e: Exception => e.printStackTrace()\n",
    "  } finally {\n",
    "      connection.close()\n",
    "  }   \n",
    "}\n",
    "\n",
    "def benchmark(name: String, dataframe: DataFrame, schema: String) = {\n",
    "    var t = System.nanoTime()\n",
    "    dataframe.write.option(\"createTableOptions\", \"ENGINE=innodb\").\n",
    "    option(\"createTableColumnTypes\", schema).jdbc(url, \"benchmark.jdbc_innodb_\"+name, connectionProperties)\n",
    "    val jdbc_innodb_time = System.nanoTime() - t\n",
    "    t = System.nanoTime()\n",
    "    dataframe.write.option(\"numPartitions\", 1).option(\"createTableOptions\", \"ENGINE=columnstore\").\n",
    "    option(\"createTableColumnTypes\", schema).jdbc(url, \"benchmark.jdbc_columnstore_\"+name, connectionProperties)\n",
    "    val jdbc_columnstore_time = System.nanoTime() - t\n",
    "    t = System.nanoTime()\n",
    "    createColumnStoreAPITable(\"api_columnstore_\"+name, schema)\n",
    "    ColumnStoreExporter.export(\"benchmark\", \"api_columnstore_\"+name, dataframe)\n",
    "    val api_columnstore_time = System.nanoTime() - t\n",
    "    (jdbc_innodb_time, jdbc_columnstore_time, api_columnstore_time)\n",
    "}\n",
    "\n",
    "val ascii_benchmark = benchmark(\"ascii\", asciiDF, \"ascii_representation CHAR(1), number INT\")\n",
    "val rand_benchmark = benchmark(\"rand\", randDF, \"id BIGINT, uniform DOUBLE, normal DOUBLE\")\n",
    "val hash_benchmark = benchmark(\"hash\", hashDF, \"number BIGINT, sha1 VARCHAR(40), sha256 VARCHAR(64), sha512 VARCHAR(128), md5 VARCHAR(32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the comparison in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"jdbc_innodb\\tjdbc_columnstore\\tapi_columnstore\\t\\trows\\t\\titems\")\n",
    "println(ascii_benchmark._1/1000000000.toDouble+\"s\\t\"+ascii_benchmark._2/1000000000.toDouble+\"s\\t\\t\"+ascii_benchmark._3/1000000000.toDouble+\"s\\t\\t\"+asciiDF.count+\"\\t\\t\"+asciiDF.count*asciiDF.columns.size)\n",
    "println(rand_benchmark._1/1000000000.toDouble+\"s\\t\"+rand_benchmark._2/1000000000.toDouble+\"s\\t\\t\"+rand_benchmark._3/1000000000.toDouble+\"s\\t\\t\"+randDF.count+\"\\t\\t\"+randDF.count*randDF.columns.size)\n",
    "println(hash_benchmark._1/1000000000.toDouble+\"s\\t\"+hash_benchmark._2/1000000000.toDouble+\"s\\t\\t\"+hash_benchmark._3/1000000000.toDouble+\"s\\t\\t\"+hashDF.count+\"\\t\\t\"+hashDF.count*hashDF.columns.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
